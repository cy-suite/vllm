Architecture Overview
======================

This document provides an overview of the vLLM architecture.

.. contents:: Table of Contents
    :local:
    :depth: 2

Entrypoints
-----------

vLLM provides a number of entrypoints for interacting with the system. The
following diagram shows the relationship between them.

.. image:: arch_overview/entrypoints.excalidraw.png
    :alt: Entrypoints Diagram

LLM Class
^^^^^^^^^

The LLM class provides the primary Python interface for doing offline inference,
which is interacting with a model without using a separate model inference
server.

Here is a sample of `LLM` class usage:

.. code-block:: python

    from vllm import LLM, SamplingParams

    # Define a list of input prompts
    prompts = [
        "Hello, my name is",
        "The capital of France is",
        "The largest ocean is",
    ]

    # Define sampling parameters
    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

    # Initialize the LLM engine with the OPT-125M model
    llm = LLM(model="Qwen/Qwen2.5-1.5B-Instruct")

    # Generate outputs for the input prompts
    outputs = llm.generate(prompts, sampling_params)

    # Print the generated outputs
    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")

More API details can be found in the :doc:`Offline Inference
</dev/offline_inference/offline_index>` section of the API docs.

The code for the `LLM` class can be found in `vllm/entrypoints/llm.py
<https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py>`_.

OpenAI-compatible API server
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The second primary interface to vLLM is via its OpenAI-compatible API server.
This server can be started using the `vllm serve` command.

.. code-block:: bash

    vllm serve <model>

The code for the `vllm` CLI can be found in `vllm/scripts.py
<https://github.com/vllm-project/vllm/blob/main/vllm/scripts.py>`_.

Sometimes you may see the API server entrypoint used directly instead of via the
`vllm` CLI command. For example:

.. code-block:: bash

    python -m vllm.entrypoints.openai.api_server --model <model>

That code can be found in `vllm/entrypoints/openai/api_server.py
<https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/openai/api_server.py>`_.

More details on the API server can be found in the :doc:`OpenAI Compatible
Server </serving/openai_compatible_server>` document.

LLM Engine
----------

The `LLMEngine` and `AsyncLLMEngine` classes are central to the functioning of
the vLLM system, handling model inference and asynchronous request processing.

.. image:: arch_overview/llm_engine.excalidraw.png
    :alt: LLMEngine Diagram

LLMEngine
^^^^^^^^^

The `LLMEngine` class is the core component of the vLLM engine. It is
responsible for receiving requests from clients and generating outputs from the
model. The `LLMEngine` includes input processing, model execution (possibly
distributed across multiple hosts and/or GPUs), scheduling, and output
processing.

- **Input Processing**: Handles tokenization of input text using the specified
  tokenizer.

- **Scheduling**: Chooses which requests are processed in each step.

- **Model Execution**: Manages the execution of the language model, including
  distributed execution across multiple GPUs.

- **Output Processing**: Processes the outputs generated by the model, decoding the
  token IDs from a language model into human-readable text.

The code for `LLMEngine` can be found in `vllm/engine/llm_engine.py`_.

.. _vllm/engine/llm_engine.py: https://github.com/vllm-project/vllm/tree/main/vllm/engine/llm_engine.py

AsyncLLMEngine
^^^^^^^^^^^^^^

The `AsyncLLMEngine` class is an asynchronous wrapper for the `LLMEngine` class.
It uses `asyncio` to create a background loop that continuously processes
incoming requests. The `AsyncLLMEngine` is designed for online serving, where it
can handle multiple concurrent requests and stream outputs to clients.

The OpenAI-compatible API server uses the `AsyncLLMEngine`. There is also a demo
API server that serves as a simpler example in
`vllm/entrypoints/api_server.py`_.

.. _vllm/entrypoints/api_server.py: https://github.com/vllm-project/vllm/tree/main/vllm/entrypoints/api_server.py

The code for `AsyncLLMEngine` can be found in `vllm/engine/async_llm_engine.py`_.

.. _vllm/engine/async_llm_engine.py: https://github.com/vllm-project/vllm/tree/main/vllm/engine/async_llm_engine.py
