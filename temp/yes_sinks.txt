INFO 04-23 04:14:51 pynccl.py:58] Loading nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 04-23 04:14:52 llm_engine.py:81] Initializing an LLM engine (v0.4.0.post1) with config: model='lmsys/vicuna-7b-v1.5', speculative_config=None, tokenizer='lmsys/vicuna-7b-v1.5', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, seed=0)
INFO 04-23 04:14:52 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.
INFO 04-23 04:14:52 selector.py:33] Using XFormers backend.
INFO 04-23 04:14:54 weight_utils.py:194] Using model weights format ['*.bin']
INFO 04-23 04:15:01 model_runner.py:164] Loading model weights took 12.5523 GB
	execute_model: token id: tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0'), positions: tensor([ 0,  1,  2,  ..., 13, 14, 15], device='cuda:0')
INFO 04-23 04:15:02 gpu_executor.py:81] # GPU blocks: 3394, # CPU blocks: 512
max model len 4096
	execute_model: token id: tensor([    1,  2266,   338,  ...,   363,   592, 29973], device='cuda:0'), positions: tensor([   0,    1,    2,  ..., 4065, 4066, 4067], device='cuda:0')
		FREE BLOCKS 3139
INFO 04-23 04:15:04 metrics.py:224] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%
	execute_model: token id: tensor([13], device='cuda:0'), positions: tensor([4068], device='cuda:0')
read key@4067	 20.0625
write key@4068	 56.90625
read key@4067	 39.53125
write key@4068	 75.625
read key@4067	 58.9375
write key@4068	 73.625
read key@4067	 63.96875
write key@4068	 70.875
read key@4067	 61.4375
write key@4068	 86.0
read key@4067	 55.5625
write key@4068	 85.3125
read key@4067	 56.21875
write key@4068	 86.75
read key@4067	 45.40625
write key@4068	 82.8125
read key@4067	 60.5
write key@4068	 81.625
read key@4067	 53.34375
write key@4068	 83.4375
read key@4067	 52.71875
write key@4068	 86.3125
read key@4067	 43.1875
write key@4068	 89.125
read key@4067	 46.90625
write key@4068	 87.8125
read key@4067	 49.59375
write key@4068	 87.0625
read key@4067	 49.03125
write key@4068	 94.75
read key@4067	 47.9375
write key@4068	 101.1875
read key@4067	 47.875
write key@4068	 107.0
read key@4067	 43.21875
write key@4068	 96.625
read key@4067	 47.6875
write key@4068	 97.3125
read key@4067	 48.96875
write key@4068	 108.75
read key@4067	 48.65625
write key@4068	 118.1875
read key@4067	 42.84375
write key@4068	 119.4375
read key@4067	 51.78125
write key@4068	 127.6875
read key@4067	 47.21875
write key@4068	 131.0
read key@4067	 55.5
write key@4068	 132.875
read key@4067	 53.71875
write key@4068	 128.25
read key@4067	 51.3125
write key@4068	 148.125
read key@4067	 54.1875
write key@4068	 142.0
read key@4067	 52.9375
write key@4068	 139.875
read key@4067	 44.8125
write key@4068	 148.875
read key@4067	 48.59375
write key@4068	 149.375
read key@4067	 53.90625
write key@4068	 64.8125
		FREE BLOCKS 3139
INFO 04-23 04:15:33 metrics.py:224] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%
	execute_model: token id: tensor([15197], device='cuda:0'), positions: tensor([4069], device='cuda:0')
read key@4068	 26.734375
write key@4069	 43.375
read key@4068	 46.125
write key@4069	 85.5
read key@4068	 59.3125
write key@4069	 77.875
read key@4068	 50.0625
write key@4069	 72.75
read key@4068	 59.0625
write key@4069	 86.6875
read key@4068	 54.09375
write key@4069	 86.25
read key@4068	 55.5625
write key@4069	 86.8125
read key@4068	 59.125
write key@4069	 83.25
read key@4068	 58.0625
write key@4069	 82.0625
read key@4068	 64.5625
write key@4069	 84.0
read key@4068	 58.09375
write key@4069	 86.6875
read key@4068	 53.09375
write key@4069	 89.375
read key@4068	 56.375
write key@4069	 88.9375
read key@4068	 65.75
write key@4069	 88.0625
read key@4068	 59.0625
write key@4069	 96.1875
read key@4068	 63.65625
write key@4069	 102.75
read key@4068	 69.375
write key@4069	 107.8125
read key@4068	 52.0625
write key@4069	 97.0
read key@4068	 61.34375
write key@4069	 97.875
read key@4068	 63.375
write key@4069	 109.4375
read key@4068	 67.0625
write key@4069	 119.125
read key@4068	 72.125
write key@4069	 120.5625
read key@4068	 74.5625
write key@4069	 128.375
read key@4068	 73.875
write key@4069	 132.625
read key@4068	 77.0
write key@4069	 135.375
read key@4068	 67.4375
write key@4069	 131.875
read key@4068	 65.9375
write key@4069	 152.625
read key@4068	 76.1875
write key@4069	 145.875
read key@4068	 74.3125
write key@4069	 144.5
read key@4068	 73.0
write key@4069	 154.625
read key@4068	 75.0625
write key@4069	 156.875
read key@4068	 53.875
write key@4069	 55.9375
		FREE BLOCKS 3139
INFO 04-23 04:16:02 metrics.py:224] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.5%, CPU KV cache usage: 0.0%
	execute_model: token id: tensor([13], device='cuda:0'), positions: tensor([4070], device='cuda:0')
read key@4069	 27.859375
write key@4070	 56.90625
read key@4069	 44.4375
write key@4070	 75.6875
read key@4069	 47.59375
write key@4070	 73.875
read key@4069	 55.5625
write key@4070	 71.0625
read key@4069	 47.625
write key@4070	 86.0625
read key@4069	 52.625
write key@4070	 85.0625
read key@4069	 54.46875
write key@4070	 86.125
read key@4069	 58.4375
write key@4070	 82.8125
read key@4069	 54.75
write key@4070	 80.9375
read key@4069	 59.625
write key@4070	 82.9375
read key@4069	 57.40625
write key@4070	 85.75
read key@4069	 56.21875
write key@4070	 88.25
read key@4069	 61.0625
write key@4070	 87.4375
read key@4069	 45.5625
write key@4070	 86.6875
read key@4069	 59.125
write key@4070	 94.75
read key@4069	 64.9375
write key@4070	 101.25
read key@4069	 67.0625
write key@4070	 106.6875
read key@4069	 65.9375
write key@4070	 96.1875
read key@4069	 65.1875
write key@4070	 97.0625
read key@4069	 54.375
write key@4070	 109.0
read key@4069	 74.4375
write key@4070	 119.375
read key@4069	 66.0625
write key@4070	 121.125
read key@4069	 77.9375
write key@4070	 128.75
read key@4069	 59.1875
write key@4070	 134.0
read key@4069	 61.59375
write key@4070	 137.25
