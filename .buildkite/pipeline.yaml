



steps:
  - label: ":docker: build image"
    key: image-build
    agents:
      queue: cpu_queue
    commands:
      - "aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/q9t5s3a7"
      - "docker build --build-arg max_jobs=16 --build-arg USE_SCCACHE=1 --tag public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT --target test --progress plain ."
      - "docker push public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT"
    env:
      DOCKER_BUILDKIT: "1"
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5

  - label: "Neuron Test"
    depends_on: ~
    agents:
      queue: neuron
    command: bash .buildkite/run-neuron-test.sh
    soft_fail: false

  - label: "Intel CPU Test"
    depends_on: ~
    agents:
      queue: intel-cpu
    command: bash .buildkite/run-cpu-test.sh

  - label: "Intel GPU Test"
    depends_on: ~
    agents:
      queue: intel-gpu
    command: bash .buildkite/run-xpu-test.sh

  
  
  - label: "Async Engine, Inputs, Utils, Worker Test"
    depends_on: image-build
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pytest -v -s async_engine && bash ../.buildkite/download-images.sh && pytest -v -s test_inputs.py && pytest -v -s multimodal && pytest -v -s test_utils.py && pytest -v -s worker"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - label: "Tensorizer, Metrics, Tracing Test"
    depends_on: image-build
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && apt-get install curl libsodium23 && pytest -v -s tensorizer_loader && pytest -v -s metrics && pip install opentelemetry-sdk opentelemetry-api opentelemetry-exporter-otlp opentelemetry-semantic-conventions-ai && pytest -v -s tracing"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - label: "Regression Test"
    depends_on: image-build
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pytest -v -s test_regression.py"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  
  
  - label: "Basic Correctness Test"
    depends_on: image-build
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.0.8/flashinfer-0.0.8+cu121torch2.3-cp310-cp310-linux_x86_64.whl && VLLM_ATTENTION_BACKEND=XFORMERS pytest -v -s basic_correctness/test_basic_correctness.py && VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s basic_correctness/test_basic_correctness.py && VLLM_ATTENTION_BACKEND=FLASHINFER pytest -v -s basic_correctness/test_basic_correctness.py && VLLM_ATTENTION_BACKEND=XFORMERS pytest -v -s basic_correctness/test_chunked_prefill.py && VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s basic_correctness/test_chunked_prefill.py && VLLM_TEST_ENABLE_ARTIFICIAL_PREEMPT=1 pytest -v -s basic_correctness/test_preemption.py"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - label: "Core Test"
    depends_on: image-build
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pytest -v -s core && pytest -v -s distributed/test_parallel_state.py"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  
  
  
  
  - label: "Distributed Tests (4 GPUs)"
    depends_on: image-build
    agents:
      
      queue: gpu_4_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pytest -v -s distributed/test_pynccl.py && TEST_DIST_MODEL=facebook/opt-125m DISTRIBUTED_EXECUTOR_BACKEND=ray pytest -v -s distributed/test_basic_distributed_correctness.py && TEST_DIST_MODEL=facebook/opt-125m DISTRIBUTED_EXECUTOR_BACKEND=mp pytest -v -s distributed/test_basic_distributed_correctness.py && pytest -v -s spec_decode/e2e/test_integration_dist_tp4.py"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  
  
  
  
  - label: "Entrypoints Test"
    depends_on: image-build
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pytest -v -s entrypoints/llm && pytest -v -s entrypoints/openai"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  - label: "Documentation Build"
    depends_on: image-build
    agents:
      
      queue: small_cpu_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          
          command: ["bash", "-c", "cd /vllm-workspace/test_docs/docs && pip install -r requirements-docs.txt && SPHINXOPTS=\"-W\" make html"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  

  
  
  
  
  
  
  
  
  - block: "Run AsyncEngine Test"
    key: block-asyncengine-test
    depends_on: image_build

  - label: "AsyncEngine Test"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pytest -v -s async_engine"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  
  
  
  
  - block: "Run Distributed Comm Ops Test"
    key: block-distributed-comm-ops-test
    depends_on: image_build

  - label: "Distributed Comm Ops Test"
    agents:
      
      queue: gpu_4_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pytest -v -s distributed/test_comm_ops.py && pytest -v -s distributed/test_shm_broadcast.py"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run Distributed Tests (2 GPUs)"
    key: block-distributed-tests-2-gpus
    depends_on: image_build

  - label: "Distributed Tests (2 GPUs)"
    agents:
      
      queue: gpu_4_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && bash ../.buildkite/download-images.sh && VLLM_TEST_SAME_HOST=1 torchrun --nproc-per-node=4 distributed/test_same_node.py && TEST_DIST_MODEL=facebook/opt-125m DISTRIBUTED_EXECUTOR_BACKEND=ray pytest -v -s distributed/test_basic_distributed_correctness.py && TEST_DIST_MODEL=meta-llama/Llama-2-7b-hf DISTRIBUTED_EXECUTOR_BACKEND=ray pytest -v -s distributed/test_basic_distributed_correctness.py && TEST_DIST_MODEL=facebook/opt-125m DISTRIBUTED_EXECUTOR_BACKEND=ray pytest -v -s distributed/test_chunked_prefill_distributed.py && TEST_DIST_MODEL=meta-llama/Llama-2-7b-hf DISTRIBUTED_EXECUTOR_BACKEND=ray pytest -v -s distributed/test_chunked_prefill_distributed.py && TEST_DIST_MODEL=llava-hf/llava-1.5-7b-hf DISTRIBUTED_EXECUTOR_BACKEND=ray pytest -v -s distributed/test_multimodal_broadcast.py && TEST_DIST_MODEL=microsoft/Phi-3-vision-128k-instruct DISTRIBUTED_EXECUTOR_BACKEND=ray pytest -v -s distributed/test_multimodal_broadcast.py && TEST_DIST_MODEL=facebook/opt-125m DISTRIBUTED_EXECUTOR_BACKEND=mp pytest -v -s distributed/test_basic_distributed_correctness.py && TEST_DIST_MODEL=meta-llama/Llama-2-7b-hf DISTRIBUTED_EXECUTOR_BACKEND=mp pytest -v -s distributed/test_basic_distributed_correctness.py && TEST_DIST_MODEL=facebook/opt-125m DISTRIBUTED_EXECUTOR_BACKEND=mp pytest -v -s distributed/test_chunked_prefill_distributed.py && TEST_DIST_MODEL=meta-llama/Llama-2-7b-hf DISTRIBUTED_EXECUTOR_BACKEND=mp pytest -v -s distributed/test_chunked_prefill_distributed.py && TEST_DIST_MODEL=llava-hf/llava-1.5-7b-hf DISTRIBUTED_EXECUTOR_BACKEND=mp pytest -v -s distributed/test_multimodal_broadcast.py && TEST_DIST_MODEL=microsoft/Phi-3-vision-128k-instruct DISTRIBUTED_EXECUTOR_BACKEND=mp pytest -v -s distributed/test_multimodal_broadcast.py && pytest -v -s spec_decode/e2e/test_integration_dist_tp2.py && CUDA_VISIBLE_DEVICES=0,1 pytest -v -s test_sharded_state_loader.py && CUDA_VISIBLE_DEVICES=0,1 pytest -v -s distributed/test_utils.py"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  
  
  - block: "Run Pipeline Parallelism Test"
    key: block-pipeline-parallelism-test
    depends_on: image_build

  - label: "Pipeline Parallelism Test"
    agents:
      
      queue: gpu_4_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && TP_SIZE=2 PP_SIZE=2 EAGER_MODE=1 CHUNKED_PREFILL=1 pytest -v -s distributed/test_pipeline_parallel.py && TP_SIZE=2 PP_SIZE=2 EAGER_MODE=1 CHUNKED_PREFILL=0 pytest -v -s distributed/test_pipeline_parallel.py && TP_SIZE=1 PP_SIZE=3 EAGER_MODE=1 CHUNKED_PREFILL=0 pytest -v -s distributed/test_pipeline_parallel.py && PP_SIZE=4 EAGER_MODE=1 CHUNKED_PREFILL=1 pytest -v -s distributed/test_pipeline_parallel.py && PP_SIZE=4 EAGER_MODE=1 CHUNKED_PREFILL=0 pytest -v -s distributed/test_pipeline_parallel.py"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run Engine Test"
    key: block-engine-test
    depends_on: image_build

  - label: "Engine Test"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pytest -v -s engine test_sequence.py test_config.py test_logger.py && pytest -v -s tokenization"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  
  
  - block: "Run Examples Test"
    key: block-examples-test
    depends_on: image_build

  - label: "Examples Test"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/examples && pip install awscli tensorizer && python3 offline_inference.py && python3 offline_inference_with_prefix.py && python3 llm_engine_example.py && python3 llava_example.py && python3 tensorize_vllm_model.py --model facebook/opt-125m serialize --serialized-directory /tmp/ --suffix v1 && python3 tensorize_vllm_model.py --model facebook/opt-125m deserialize --path-to-tensors /tmp/vllm/facebook/opt-125m/v1/model.tensors"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run Inputs Test"
    key: block-inputs-test
    depends_on: image_build

  - label: "Inputs Test"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && bash ../.buildkite/download-images.sh && pytest -v -s test_inputs.py && pytest -v -s multimodal"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run Kernels Test %N"
    key: block-kernels-test-n
    depends_on: image_build

  - label: "Kernels Test %N"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    parallelism: 4
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.0.8/flashinfer-0.0.8+cu121torch2.3-cp310-cp310-linux_x86_64.whl && pytest -v -s kernels --shard-id=$$BUILDKITE_PARALLEL_JOB --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run Models Test"
    key: block-models-test
    depends_on: image_build

  - label: "Models Test"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.0.8/flashinfer-0.0.8+cu121torch2.3-cp310-cp310-linux_x86_64.whl && pytest -v -s models -m \"not vlm\""]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run Vision Language Models Test"
    key: block-vision-language-models-test
    depends_on: image_build

  - label: "Vision Language Models Test"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && bash ../.buildkite/download-images.sh && pytest -v -s models -m vlm"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run Prefix Caching Test"
    key: block-prefix-caching-test
    depends_on: image_build

  - label: "Prefix Caching Test"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pytest -v -s prefix_caching"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run Samplers Test"
    key: block-samplers-test
    depends_on: image_build

  - label: "Samplers Test"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pytest -v -s samplers"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run LogitsProcessor Test"
    key: block-logitsprocessor-test
    depends_on: image_build

  - label: "LogitsProcessor Test"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pytest -v -s test_logits_processor.py"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run Utils Test"
    key: block-utils-test
    depends_on: image_build

  - label: "Utils Test"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pytest -v -s test_utils.py"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run Worker Test"
    key: block-worker-test
    depends_on: image_build

  - label: "Worker Test"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pytest -v -s worker"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run Speculative decoding tests"
    key: block-speculative-decoding-tests
    depends_on: image_build

  - label: "Speculative decoding tests"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && export VLLM_ATTENTION_BACKEND=XFORMERS && pytest -v -s spec_decode"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
            - VLLM_ATTENTION_BACKEND=XFORMERS
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run LoRA Test %N"
    key: block-lora-test-n
    depends_on: image_build

  - label: "LoRA Test %N"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    parallelism: 4
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pytest -v -s lora --shard-id=$$BUILDKITE_PARALLEL_JOB --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT --ignore=lora/test_long_context.py"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run LoRA Long Context (Distributed)"
    key: block-lora-long-context-distributed
    depends_on: image_build

  - label: "LoRA Long Context (Distributed)"
    agents:
      
      queue: gpu_4_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && export VLLM_WORKER_MULTIPROC_METHOD=spawn && pytest -v -s -x lora/test_long_context.py"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run Tensorizer Test"
    key: block-tensorizer-test
    depends_on: image_build

  - label: "Tensorizer Test"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && apt-get install curl libsodium23 && pytest -v -s tensorizer_loader"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run Metrics Test"
    key: block-metrics-test
    depends_on: image_build

  - label: "Metrics Test"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pytest -v -s metrics"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run Quantization Test"
    key: block-quantization-test
    depends_on: image_build

  - label: "Quantization Test"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pytest -v -s quantization"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run Tracing Test"
    key: block-tracing-test
    depends_on: image_build

  - label: "Tracing Test"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/tests && pip install opentelemetry-sdk opentelemetry-api opentelemetry-exporter-otlp opentelemetry-semantic-conventions-ai && pytest -v -s tracing"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run Benchmarks"
    key: block-benchmarks
    depends_on: image_build

  - label: "Benchmarks"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          mount-buildkite-agent: true
          
          command: ["bash", "-c", "cd /vllm-workspace/.buildkite && pip install aiohttp && bash run-benchmarks.sh"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  - block: "Run LM Eval Small Models"
    key: block-lm-eval-small-models
    depends_on: image_build

  - label: "LM Eval Small Models"
    agents:
      
      queue: gpu_1_queue
      
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
      - docker#v5.2.0:
          image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT
          always-pull: true
          propagate-environment: true
          
          gpus: all
          
          
          command: ["bash", "-c", "cd /vllm-workspace/.buildkite/lm-eval-harness && pip install lm-eval && export VLLM_WORKER_MULTIPROC_METHOD=spawn && bash ./run-tests.sh -c configs/models-small.txt -t 1"]
          environment:
            - VLLM_USAGE_SOURCE=ci-test
            - HF_HOME=/root/.cache/huggingface
            - HF_TOKEN
            
          volumes:
            - /dev/shm:/dev/shm
            - /root/.cache/huggingface:/root/.cache/huggingface
  
  
  
  
  
  
  
  

  - block: "Run A100 tests"
    depends_on: image-build

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  - label: "LM Eval Large Models"
    priority: 10000
    agents:
      queue: a100-queue
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
    - kubernetes:
        podSpec:
          priorityClassName: ci
          containers:
          - image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:f17f03744ebabed187634baec601ef35094ae14f
            command: ["bash"]
            args:
            - '-c'
            - "'cd /vllm-workspace/.buildkite/lm-eval-harness && pip install lm-eval && export VLLM_WORKER_MULTIPROC_METHOD=spawn && bash ./run-tests.sh -c configs/models-large.txt -t 4'"
            resources:
              limits:
                nvidia.com/gpu: 4
            volumeMounts:
            - name: devshm
              mountPath: /dev/shm
            - name: hf-cache
              mountPath: /root/.cache/huggingface
            env:
            - name: VLLM_USAGE_SOURCE
              value: ci-test
            - name: HF_HOME
              value: /root/.cache/huggingface
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: token
          nodeSelector:
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          volumes:
          - name: devshm
            emptyDir:
              medium: Memory
          - name: hf-cache
            hostPath:
              path: /root/.cache/huggingface
              type: Directory
  
  
  
  
  
  - label: "Distributed Tests (A100)"
    priority: 10000
    agents:
      queue: a100-queue
    soft_fail: false
    
    retry:
      automatic:
        - exit_status: -1  # Agent was lost
          limit: 5
        - exit_status: -10  # Agent was lost
          limit: 5
    plugins:
    - kubernetes:
        podSpec:
          priorityClassName: ci
          containers:
          - image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:f17f03744ebabed187634baec601ef35094ae14f
            command: ["bash"]
            args:
            - '-c'
            - "'cd /vllm-workspace/tests && pytest -v -s distributed/test_custom_all_reduce.py && TEST_DIST_MODEL=facebook/opt-125m DISTRIBUTED_EXECUTOR_BACKEND=ray pytest -v -s distributed/test_basic_distributed_correctness.py && TEST_DIST_MODEL=facebook/opt-125m DISTRIBUTED_EXECUTOR_BACKEND=mp pytest -v -s distributed/test_basic_distributed_correctness.py && pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.0.8/flashinfer-0.0.8+cu121torch2.3-cp310-cp310-linux_x86_64.whl && VLLM_ATTENTION_BACKEND=FLASHINFER TEST_DIST_MODEL=facebook/opt-125m DISTRIBUTED_EXECUTOR_BACKEND=ray pytest -v -s distributed/test_basic_distributed_correctness.py && VLLM_ATTENTION_BACKEND=FLASHINFER TEST_DIST_MODEL=meta-llama/Meta-Llama-3-8B DISTRIBUTED_EXECUTOR_BACKEND=ray pytest -v -s distributed/test_basic_distributed_correctness.py && pytest -v -s -x lora/test_mixtral.py'"
            resources:
              limits:
                nvidia.com/gpu: 4
            volumeMounts:
            - name: devshm
              mountPath: /dev/shm
            - name: hf-cache
              mountPath: /root/.cache/huggingface
            env:
            - name: VLLM_USAGE_SOURCE
              value: ci-test
            - name: HF_HOME
              value: /root/.cache/huggingface
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: token
          nodeSelector:
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          volumes:
          - name: devshm
            emptyDir:
              medium: Memory
          - name: hf-cache
            hostPath:
              path: /root/.cache/huggingface
              type: Directory
  
  

  - block: "Run AMD tests"
    key: block-amd-tests
    depends_on: ~

  - group: "AMD Tests"
    depends_on: block-amd-tests
    steps:
    
    
    
    
    
    
      - label: "AMD: Regression Test"
        agents:
          queue: amd
        command: bash .buildkite/run-amd-test.sh "cd /vllm-workspace/tests ; pytest -v -s test_regression.py"
        env:
          DOCKER_BUILDKIT: "1"
        priority: 100
        soft_fail: true
    
    
    
    
    
      - label: "AMD: Basic Correctness Test"
        agents:
          queue: amd
        command: bash .buildkite/run-amd-test.sh "cd /vllm-workspace/tests ; pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.0.8/flashinfer-0.0.8+cu121torch2.3-cp310-cp310-linux_x86_64.whl && VLLM_ATTENTION_BACKEND=XFORMERS pytest -v -s basic_correctness/test_basic_correctness.py && VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s basic_correctness/test_basic_correctness.py && VLLM_ATTENTION_BACKEND=FLASHINFER pytest -v -s basic_correctness/test_basic_correctness.py && VLLM_ATTENTION_BACKEND=XFORMERS pytest -v -s basic_correctness/test_chunked_prefill.py && VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s basic_correctness/test_chunked_prefill.py && VLLM_TEST_ENABLE_ARTIFICIAL_PREEMPT=1 pytest -v -s basic_correctness/test_preemption.py"
        env:
          DOCKER_BUILDKIT: "1"
        priority: 100
        soft_fail: true
    
    
    
      - label: "AMD: Core Test"
        agents:
          queue: amd
        command: bash .buildkite/run-amd-test.sh "cd /vllm-workspace/tests ; pytest -v -s core && pytest -v -s distributed/test_parallel_state.py"
        env:
          DOCKER_BUILDKIT: "1"
        priority: 100
        soft_fail: true
    
    
    
    
    
      - label: "AMD: Distributed Tests (2 GPUs)"
        agents:
          queue: amd
        command: bash .buildkite/run-amd-test.sh "cd /vllm-workspace/tests ; bash ../.buildkite/download-images.sh && VLLM_TEST_SAME_HOST=1 torchrun --nproc-per-node=4 distributed/test_same_node.py && TEST_DIST_MODEL=facebook/opt-125m DISTRIBUTED_EXECUTOR_BACKEND=ray pytest -v -s distributed/test_basic_distributed_correctness.py && TEST_DIST_MODEL=meta-llama/Llama-2-7b-hf DISTRIBUTED_EXECUTOR_BACKEND=ray pytest -v -s distributed/test_basic_distributed_correctness.py && TEST_DIST_MODEL=facebook/opt-125m DISTRIBUTED_EXECUTOR_BACKEND=ray pytest -v -s distributed/test_chunked_prefill_distributed.py && TEST_DIST_MODEL=meta-llama/Llama-2-7b-hf DISTRIBUTED_EXECUTOR_BACKEND=ray pytest -v -s distributed/test_chunked_prefill_distributed.py && TEST_DIST_MODEL=llava-hf/llava-1.5-7b-hf DISTRIBUTED_EXECUTOR_BACKEND=ray pytest -v -s distributed/test_multimodal_broadcast.py && TEST_DIST_MODEL=microsoft/Phi-3-vision-128k-instruct DISTRIBUTED_EXECUTOR_BACKEND=ray pytest -v -s distributed/test_multimodal_broadcast.py && TEST_DIST_MODEL=facebook/opt-125m DISTRIBUTED_EXECUTOR_BACKEND=mp pytest -v -s distributed/test_basic_distributed_correctness.py && TEST_DIST_MODEL=meta-llama/Llama-2-7b-hf DISTRIBUTED_EXECUTOR_BACKEND=mp pytest -v -s distributed/test_basic_distributed_correctness.py && TEST_DIST_MODEL=facebook/opt-125m DISTRIBUTED_EXECUTOR_BACKEND=mp pytest -v -s distributed/test_chunked_prefill_distributed.py && TEST_DIST_MODEL=meta-llama/Llama-2-7b-hf DISTRIBUTED_EXECUTOR_BACKEND=mp pytest -v -s distributed/test_chunked_prefill_distributed.py && TEST_DIST_MODEL=llava-hf/llava-1.5-7b-hf DISTRIBUTED_EXECUTOR_BACKEND=mp pytest -v -s distributed/test_multimodal_broadcast.py && TEST_DIST_MODEL=microsoft/Phi-3-vision-128k-instruct DISTRIBUTED_EXECUTOR_BACKEND=mp pytest -v -s distributed/test_multimodal_broadcast.py && pytest -v -s spec_decode/e2e/test_integration_dist_tp2.py && CUDA_VISIBLE_DEVICES=0,1 pytest -v -s test_sharded_state_loader.py && CUDA_VISIBLE_DEVICES=0,1 pytest -v -s distributed/test_utils.py"
        env:
          DOCKER_BUILDKIT: "1"
        priority: 100
        soft_fail: true
    
    
    
    
    
    
    
      - label: "AMD: Engine Test"
        agents:
          queue: amd
        command: bash .buildkite/run-amd-test.sh "cd /vllm-workspace/tests ; pytest -v -s engine test_sequence.py test_config.py test_logger.py && pytest -v -s tokenization"
        env:
          DOCKER_BUILDKIT: "1"
        priority: 100
        soft_fail: true
    
    
    
      - label: "AMD: Entrypoints Test"
        agents:
          queue: amd
        command: bash .buildkite/run-amd-test.sh "cd /vllm-workspace/tests ; pytest -v -s entrypoints/llm && pytest -v -s entrypoints/openai"
        env:
          DOCKER_BUILDKIT: "1"
        priority: 100
        soft_fail: true
    
    
    
      - label: "AMD: Examples Test"
        agents:
          queue: amd
        command: bash .buildkite/run-amd-test.sh "cd /vllm-workspace/examples ; pip install awscli tensorizer && python3 offline_inference.py && python3 offline_inference_with_prefix.py && python3 llm_engine_example.py && python3 llava_example.py && python3 tensorize_vllm_model.py --model facebook/opt-125m serialize --serialized-directory /tmp/ --suffix v1 && python3 tensorize_vllm_model.py --model facebook/opt-125m deserialize --path-to-tensors /tmp/vllm/facebook/opt-125m/v1/model.tensors"
        env:
          DOCKER_BUILDKIT: "1"
        priority: 100
        soft_fail: true
    
    
    
    
    
    
    
    
    
      - label: "AMD: Vision Language Models Test"
        agents:
          queue: amd
        command: bash .buildkite/run-amd-test.sh "cd /vllm-workspace/tests ; bash ../.buildkite/download-images.sh && pytest -v -s models -m vlm"
        env:
          DOCKER_BUILDKIT: "1"
        priority: 100
        soft_fail: true
    
    
    
      - label: "AMD: Prefix Caching Test"
        agents:
          queue: amd
        command: bash .buildkite/run-amd-test.sh "cd /vllm-workspace/tests ; pytest -v -s prefix_caching"
        env:
          DOCKER_BUILDKIT: "1"
        priority: 100
        soft_fail: true
    
    
    
    
    
      - label: "AMD: LogitsProcessor Test"
        agents:
          queue: amd
        command: bash .buildkite/run-amd-test.sh "cd /vllm-workspace/tests ; pytest -v -s test_logits_processor.py"
        env:
          DOCKER_BUILDKIT: "1"
        priority: 100
        soft_fail: true
    
    
    
    
    
      - label: "AMD: Worker Test"
        agents:
          queue: amd
        command: bash .buildkite/run-amd-test.sh "cd /vllm-workspace/tests ; pytest -v -s worker"
        env:
          DOCKER_BUILDKIT: "1"
        priority: 100
        soft_fail: true
    
    
    
    
    
    
    
    
    
    
    
      - label: "AMD: Metrics Test"
        agents:
          queue: amd
        command: bash .buildkite/run-amd-test.sh "cd /vllm-workspace/tests ; pytest -v -s metrics"
        env:
          DOCKER_BUILDKIT: "1"
        priority: 100
        soft_fail: true
    
    
    
    
    
    
    
      - label: "AMD: Benchmarks"
        agents:
          queue: amd
        command: bash .buildkite/run-amd-test.sh "cd /vllm-workspace/.buildkite ; pip install aiohttp && bash run-benchmarks.sh"
        env:
          DOCKER_BUILDKIT: "1"
        priority: 100
        soft_fail: true
    
    
    
    
    
    
    
    
    
    
