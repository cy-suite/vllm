import json
import os
from typing import Dict, Optional

import torch
import torch.distributed as dist

import vllm.envs as envs
from vllm.logger import init_logger

from .parallel_state import get_cpu_world_group, get_local_rank

logger = init_logger(__name__)


import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import os
import sys

def producer(i):
    # launch process, discard output
    sys.stdout = open(os.devnull, "w")
    sys.stderr = open(os.devnull, "w")
    dist.init_process_group(
        backend="gloo",
        init_method="tcp://127.0.0.1:12345",
        world_size=2,
        rank=0,
    )
    # produce a tensor in GPU i
    data = torch.zeros((128,), device=f"cuda:{i}")
    func, args = torch.multiprocessing.reductions.reduce_tensor(data)
    args = list(args)
    dist.broadcast_object_list([(func, args)], src=0)
    dist.barrier()
    torch.cuda.synchronize()
    assert data.mean().item() == 1

def consumer(j):
    # launch process, discard output
    sys.stdout = open(os.devnull, "w")
    sys.stderr = open(os.devnull, "w")
    dist.init_process_group(
        backend="gloo",
        init_method="tcp://127.0.0.1:12345",
        world_size=2,
        rank=1,
    )
    torch.cuda.set_device(j)
    recv = [None]
    dist.broadcast_object_list(recv, src=0)
    func, args = recv[0]
    args[6] = j
    data = func(*args)
    data += 1
    dist.barrier()
    torch.cuda.synchronize()
    assert data.mean().item() == 1

def can_actually_p2p(i, j):
    """
    Usually, checking if P2P access is enabled is done by
    `torch.cuda.can_device_access_peer(i, j)`. However, on some platforms,
    the driver might return True even if P2P access is not actually possible.
    See https://github.com/vllm-project/vllm/issues/2728
    Therefore, we have to perform a real P2P access to check if it is actually
    possible.

    Note on p2p and cuda IPC:
    Usually, one process uses one GPU:
    GPU i --> cuda context i --> tensor i --> process i

    We need to combine p2p and cuda IPC, so that:
    GPU i --> cuda context i --> tensor i --> process i
                                 |shared|
    GPU j --> cuda context j --> tensor j --> process j
    """
    pi = mp.Process(target=producer, args=(i,))
    pj = mp.Process(target=consumer, args=(j,))
    pi.start()
    pj.start()
    pi.join()
    pj.join()
    return pi.exitcode == 0 and pj.exitcode == 0


# why do we need this cache?
# 1. we can have runtime checks for P2P access, where every process checks
#  P2P access to all other GPUs. Unfortunately, the test might cost many
#  (world_size * world_size) cuda context, and reduce the memory available
#  for the model. see https://github.com/vllm-project/vllm/issues/3821
# 2. alternatively, we can have a p2p map that is generated by the master
#  process and broadcasted to all other processes. This still requires
#  #world_size of cuda context, belonging to the master process, on each GPU.
# 3. we can have a cache file, that records the p2p access status. The first
#  time the master process checks the p2p access, it will generate the cache
#  file, at the cost of #world_size of cuda context. Later on, all processes
#  can read the cache file to check the p2p access status without any cost of
#  additional cuda context.
# Note that the cache file is suffixed by the CUDA_VISIBLE_DEVICES, so that we
#  can have different cache files for different CUDA_VISIBLE_DEVICES settings,
#  e.g. used by different vllm engines. The device id in the cache file is a
#  **local** device id, i.e. from 0 to num_dev-1, where num_dev is the number
#  of visible devices in the vllm engine.
_gpu_p2p_access_cache: Optional[Dict[str, bool]] = None


def gpu_p2p_access_check(i: int, j: int) -> bool:
    """Check if GPU i can access GPU j."""

    # if the cache variable is already calculated,
    # read from the cache instead of checking it again
    global _gpu_p2p_access_cache
    if _gpu_p2p_access_cache is not None:
        return _gpu_p2p_access_cache[f"{i}->{j}"]

    is_distributed = dist.is_initialized()

    num_dev = torch.cuda.device_count()
    cuda_visible_devices = envs.CUDA_VISIBLE_DEVICES
    if cuda_visible_devices is None:
        cuda_visible_devices = ",".join(str(i) for i in range(num_dev))
    VLLM_CONFIG_ROOT = envs.VLLM_CONFIG_ROOT
    path = os.path.expanduser(
        f"{VLLM_CONFIG_ROOT}/vllm/gpu_p2p_access_cache_for_{cuda_visible_devices}.json"
    )
    os.makedirs(os.path.dirname(path), exist_ok=True)
    if (not is_distributed or get_local_rank() == 0) \
        and (not os.path.exists(path)):
        # only the local master process (with local_rank == 0) can
        #  enter this block to calculate the cache
        logger.info("generating GPU P2P access cache for in %s", path)
        cache = {}
        for _i in range(num_dev):
            for _j in range(num_dev):
                cache[f"{_i}->{_j}"] = can_actually_p2p(_i, _j)
        with open(path, "w") as f:
            json.dump(cache, f, indent=4)
    if is_distributed:
        cpu_world_group = get_cpu_world_group()
        dist.barrier(cpu_world_group)
    logger.info("reading GPU P2P access cache from %s", path)
    with open(path, "r") as f:
        cache = json.load(f)
    _gpu_p2p_access_cache = cache
    return _gpu_p2p_access_cache[f"{i}->{j}"]
